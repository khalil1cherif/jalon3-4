{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f026299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pickle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c1ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3cebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
    "    return text_processed\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_text_list = list()\n",
    "    \n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
    "        else:\n",
    "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
    "    \n",
    "    return \" \".join(lemmatized_text_list)\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return \" \".join([word.lower() for word in text.split()])\n",
    "def contraction_text(text):\n",
    "    return contractions.fix(text)\n",
    "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
    "negative_prefix = \"NOT_\"\n",
    "def get_negative_token(text):\n",
    "    tokens = text.split()\n",
    "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
    "    for idx in negative_idx:\n",
    "        if idx < len(tokens):\n",
    "            tokens[idx]= negative_prefix + tokens[idx]\n",
    "    \n",
    "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
    "    \n",
    "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e403ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Tokenize review\n",
    "    text = tokenize_text(text)\n",
    "    \n",
    "    # Lemmatize review\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    # Normalize review\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # Remove contractions\n",
    "    text = contraction_text(text)\n",
    "\n",
    "    # Get negative tokens\n",
    "    text = get_negative_token(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fe6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_df=pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5221431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 24s\n",
      "Wall time: 2min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've only had food from here once and it wasn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>food memorable panang curry balance flavor lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will never return here again.  Ever.  I was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NOT_return sit booth wait dinner come scurry m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wish my experience was great as others. I di...</td>\n",
       "      <td>1</td>\n",
       "      <td>wish experience great din wednesday night week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are the rosemary grapefruit scones supposed to...</td>\n",
       "      <td>1</td>\n",
       "      <td>rosemary grapefruit scone suppose taste like b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our takeout order was half wrong. Food was mis...</td>\n",
       "      <td>1</td>\n",
       "      <td>takeout order half wrong food miss portion siz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>I was a loyal fan of Aroy before the ownership...</td>\n",
       "      <td>5</td>\n",
       "      <td>loyal fan aroy ownership change apprehensive v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Stopped here for a bite while wandering around...</td>\n",
       "      <td>5</td>\n",
       "      <td>stopped bite wander faneuil hall pleasantly su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>A quiet place with excellent food, great music...</td>\n",
       "      <td>5</td>\n",
       "      <td>quiet place excellent food great music helpful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Super delicious food. Awesome vibe. I suffered...</td>\n",
       "      <td>5</td>\n",
       "      <td>super delicious food awesome vibe suffer disne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>I have a lot of dietary restrictions and this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>lot dietary restriction place spot superfood s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  stars  \\\n",
       "0      I've only had food from here once and it wasn'...      1   \n",
       "1      I will never return here again.  Ever.  I was ...      1   \n",
       "2      I wish my experience was great as others. I di...      1   \n",
       "3      Are the rosemary grapefruit scones supposed to...      1   \n",
       "4      Our takeout order was half wrong. Food was mis...      1   \n",
       "...                                                  ...    ...   \n",
       "24995  I was a loyal fan of Aroy before the ownership...      5   \n",
       "24996  Stopped here for a bite while wandering around...      5   \n",
       "24997  A quiet place with excellent food, great music...      5   \n",
       "24998  Super delicious food. Awesome vibe. I suffered...      5   \n",
       "24999  I have a lot of dietary restrictions and this ...      5   \n",
       "\n",
       "                                            text_cleaned  \n",
       "0      food memorable panang curry balance flavor lik...  \n",
       "1      NOT_return sit booth wait dinner come scurry m...  \n",
       "2      wish experience great din wednesday night week...  \n",
       "3      rosemary grapefruit scone suppose taste like b...  \n",
       "4      takeout order half wrong food miss portion siz...  \n",
       "...                                                  ...  \n",
       "24995  loyal fan aroy ownership change apprehensive v...  \n",
       "24996  stopped bite wander faneuil hall pleasantly su...  \n",
       "24997  quiet place excellent food great music helpful...  \n",
       "24998  super delicious food awesome vibe suffer disne...  \n",
       "24999  lot dietary restriction place spot superfood s...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dataset_df[\"text_cleaned\"] = dataset_df[\"text\"].apply(preprocess_text)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773a12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2e6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c444a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08fc6bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_df=0.75, min_df=0.01)\n"
     ]
    }
   ],
   "source": [
    "vectoriseur_pickle=open('C:/Users/USER/Desktop/ESEO COURS/vectoriseur_file','rb')\n",
    "vectoriseur=load(vectoriseur_pickle)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e67feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF(n_components=15)\n"
     ]
    }
   ],
   "source": [
    "model_pickle=open('C:/Users/USER/Desktop/ESEO COURS/model_file','rb')\n",
    "model=load(model_pickle)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211d474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics1={0:'les perssonels et les tables',\n",
    "       1:'mauvaise gout des plats greek',\n",
    "       2:'mauvaise pizza et retard de livraison',\n",
    "       3:'retard de pre-commmande et de commande',\n",
    "       4:'Qualite des repas et des serveurs ne sont pas au attendu',\n",
    "       5:'mauvais endroit',\n",
    "       6:'Burger',\n",
    "       7:'beaucoup attente',\n",
    "       8:'les poulets et les salades ne sont pas a la hauteur',\n",
    "       9:'mauvais bar et mauvaise boisson',\n",
    "       10:'prix elevé par rapport a la quantité',\n",
    "       11:'livraison',\n",
    "       12:'sandwich',\n",
    "       13:'suchi',\n",
    "       14:'mauvais environnement'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2ec001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "text_neg=[\"I've only had food from here once and it wasnt good at all\"]\n",
    "text_pos=[\"I have a lot of dietary restrictions and this i very liked\"]\n",
    "print(type(text_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f656a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.04917078 0.         0.         0.16155017 0.\n",
      "  0.00053345 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text=x.transform(text_neg)\n",
    "top = y.transform(text)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524d67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.04917078 0.         0.         0.16155017 0.\n",
      "  0.00053345 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text2=x.transform(text_pos)\n",
    "top2 = y.transform(text)\n",
    "print(top2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd04df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "test=TextBlob(\"I have a lot of dietary restrictions and this i very liked\")\n",
    "test.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80594812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2=TextBlob(\"I've only had food from here once and it wasnt good at all\")\n",
    "test2.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8453f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8848d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics(model, vectorizer, n_topics, text):\n",
    "        polarity=TextBlob(text).sentiment.polarity\n",
    "        if polarity<0:\n",
    "            text=preprocess_text(text)\n",
    "            text=[text]\n",
    "            vectorized=vectorizer.transform(text)\n",
    "            topics_correlations=model.transform(vectorized)\n",
    "            unsorted_topics_correlations=topics_correlations[0].copy()\n",
    "            topics_correlations[0].sort()\n",
    "            sorted=topics_correlations[0][::-1]\n",
    "            print(sorted)\n",
    "            topics=[]\n",
    "            for i in range(n_topics):\n",
    "                corr_value= sorted[i]\n",
    "                result = np.where(unsorted_topics_correlations == corr_value)[0]\n",
    "                topics.append(topics1.get(result[0]))\n",
    "            print(topics)\n",
    "        else:\n",
    "            return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eedb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07636051 0.0194393  0.00904017 0.00832783 0.00337243 0.00244721\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "['Qualite des repas et des serveurs ne sont pas au attendu', 'suchi', 'livraison', 'sandwich', 'mauvaise pizza et retard de livraison']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict_topics(model,vectoriseur,5,\"succhi very bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0d774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65449fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
